<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://seungyounshin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://seungyounshin.github.io/" rel="alternate" type="text/html" hreflang="ko"/><updated>2025-10-27T15:05:33+09:00</updated><id>https://seungyounshin.github.io/feed.xml</id><title type="html">Seungyoun Shin(신승윤)</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Mixture of Experts LLM (MoE)</title><link href="https://seungyounshin.github.io/blog/2023/MoE/" rel="alternate" type="text/html" title="Mixture of Experts LLM (MoE)"/><published>2023-12-30T00:00:00+09:00</published><updated>2023-12-30T00:00:00+09:00</updated><id>https://seungyounshin.github.io/blog/2023/MoE</id><content type="html" xml:base="https://seungyounshin.github.io/blog/2023/MoE/"><![CDATA[<h2 id="moe가-왜-필요한가">MoE가 왜 필요한가?</h2> <p>Transformers 구조에서 메모리와 시간복잡도 모두 Attention 구조의 착안하여 증가한다. 그렇기 때문에 LLM에서 Self Attention 이 연산에서 큰 부분을 차지한다고 생각하기 쉽다. 하지만 <strong>실제 파라미터 수가 더 많고 연산에 꽤 많은 부분을 차지하는것은 Feed Forward Network</strong> 부분이다.</p> <p><a href="https://huggingface.co/meta-llama/Llama-2-13b">LLama2-13B</a> 의 <code class="language-plaintext highlighter-rouge">LlamaDecoderLayer</code>은 Self Attention 과 MLP (Feed Forward) 두개로 나누어져 있고 각각 파라미터 수를 계산하면 다음과 같다.</p> <table> <thead> <tr> <th style="text-align: center">LLama2-13B</th> <th style="text-align: center">LlamaAttention</th> <th style="text-align: center">LlamaMLP</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">13.02B</td> <td style="text-align: center">4.19B</td> <td style="text-align: center"><strong>8.49B</strong></td> </tr> </tbody> </table> <p>이와 같이 Llama 에서 MLP 가 Attention의 모든 query,key,value 파라미터 보다도 <strong>2배</strong> 가까이 많은것을 알 수 있다.</p> <p>즉, Attention 보다도 <strong>Feed Foward</strong> 레이어를 최적화하는것이 전체적인 throughput 향상에 도움이 될 수 있다.</p> <h2 id="mixtral-8x7b">Mixtral 8x7B</h2> <p>그렇다면, MoE 로 핫한 Mistral 은 어떤가? <a href="https://mistral.ai/news/mixtral-of-experts/">Mistral 8x7B</a> 도 똑같이 구해보자.</p> <table> <thead> <tr> <th style="text-align: center">Mistral 8x7B</th> <th style="text-align: center">MixtralAttention</th> <th style="text-align: center">MixtralMoeBlock</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">46.70B</td> <td style="text-align: center">1.34B</td> <td style="text-align: center"><strong>45.10B</strong></td> </tr> </tbody> </table> <p>Mistral 의 경우 MixtralMoeBlock 의 8개의 Experts 가 파라미터가 MistralAttention 보다 무려 <strong>33배</strong>가 많다. 실제 여기서 2개의 Expert 를 사용하니 Inference 자체에 사용되는 양은 $33/4 \approx 8$ 배 정도일것이다.</p> <p>물론 Mixtral 8x7B 는 조금 다른 방식의 Scaling 을 쓰고있다.</p> <p>우선, <a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a> 논문의 e.q:2.1 을 보면,</p> \[d_{\text{attn}} = d_{\text{ff}}/4 = d_{\text{model}}\] <p>위 와 같이 Feed Foward 의 중간 차원 $d_{\text{ff}}$ 은 전체 모델 및 Attention 의 차원 $d_{\text{attn}}, d_{\text{model}}$ 에 1/4 가 되게 설정한 채로 Scaling 을 진행한다.</p> <p>Llama2 와 Mixtral 8x7B의 $d_{\text{ff}}/d_{\text{model}}$ 를 계산해보자.</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">$d_{\text{ff}}$</th> <th style="text-align: center">$d_{\text{model}}$</th> <th style="text-align: center">$d_{\text{ff}}/d_{\text{model}}$</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://arxiv.org/pdf/2001.08361.pdf">Kaplan et al.</a></td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">4</td> </tr> <tr> <td style="text-align: center">Llama2 13B</td> <td style="text-align: center">13824</td> <td style="text-align: center">5120</td> <td style="text-align: center">2.7</td> </tr> <tr> <td style="text-align: center">Mixtral 8x7B</td> <td style="text-align: center">14336</td> <td style="text-align: center">4096</td> <td style="text-align: center">3.5</td> </tr> </tbody> </table> <p>하지만 Mixtral 의 경우 Query 와 Key Projection 의 Output 차원이 다르므로 Query Output Dimension 을 기준으로 계산하였다. 결과적으로는 Mixtral 도 $d_{\text{ff}}/d_{\text{model}}$ 가 3.5 로 4보다 작다.</p> <p>Llama2와 확연히 다른점은 Attention 쪽 파라미터가 더 작으면서 (Llama2-13B는 4.19B, Mixtral 8x7B 은 1.34B) Feed Forward 의 $d_{\text{ff}}$ 를 Scaling 한 것이다. 이러한 구조를 통해 Llama2 보다 더 좋은 성능을 낸다고 리포트하고 있다.</p> <p><img src="/assets/img/mixtral_perf.png" alt="Mixtral Performance" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/></p> <p>이 Figure 가 가장 전반적인 성능을 볼 수 있는 것 같아 가지고 왔다. Mixtral 8x7B는 Active Paramater 가 12B (실제 2개의 Experts만 선택을 하니까) 인데 대충 Llama13B와 비교해도 성능이 높고 70B보다도 좋다는것을 강조하고 싶었던것 같다.</p> <p>Mixtral 8x7B가 전체 Param 이 46B 정도인데 70B Llama2 보다 <strong>Code, Math 같이 Reasoning</strong> 이 많이 필요한 곳에서도 월등히 잘하는것은 결과에 조금 의구심이 들기도한다. 물론 CodeLlama 가 Code 부분에서 더 잘하는것을 생각해보면 벤치마킹을 위해 데이터셋을 보강하고 집중적으로 훈련시키면 불가능한 부분은 아니라는 생각이 들긴한다.</p> <p>몇가지 더 궁금한 점은</p> <ul> <li>우선, <strong>모든 Experts 를 다 쓰도록</strong> Inference 하면 성능이 더 떨어지는지</li> <li>Knowledge, MATH, Code … 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다.</li> </ul> <p>그런데 그런 실험 결과를 리포트하지 않은것을 보면 어쩌면 Experts 를 다 쓰게하면 성능이 떨어지는것이 아닐까 싶다. 혹시나 해서 🤗HF 의 <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions?status=open">discussions</a> 를 찾아보았는데 Experts 를 다 쓰는 경우에 대한 성능리포트를 대신 해준 사람은 없는것 같다.</p> <h2 id="routing-analysis">Routing Analysis</h2> <p><a href="https://arxiv.org/pdf/2401.04088.pdf">Mixtral of Experts</a> 라는 제목으로 논문이 공개가 되었고 위의 두번째 질문 (“Knowledge, MATH, Code 등 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다. “) 에 대해서 <strong>Routing Analysis</strong> 에서 분석하고 있어 관련 내용을 정리해보았다.</p> <p><img src="/assets/img/mixtral_fig7.png" alt="Figure 7" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/></p> <p><a href="https://arxiv.org/abs/2101.00027">Pile</a> 데이터셋에서 전체 시퀀스를 넣고 각각 도메인 (Arxiv, Github 등등) 에서 각각 Experts 들이 얼마나 Selection 되는지를 정리한 도식이다. 회색 dash line 이 1/8 이어서 이 회색선 위에 위치한것은 그만큼 많이 선택된 Experts 라는 것이다. 수학($\texttt{DM Mathematics}$) 는 31번째 Layer 에서 압도적으로 0번 Expert 에 의해 선택된것을 확인해볼 수 있다. 반대로 똑같이 31번째 Layer 에서 언어가 좀더 많은 데이터셋인 $\texttt{StackExchange},\texttt{Wikipedia}$ 에서는 8번째 Expert가 선택된것도 재밌다. 또 0번째 Layer 에서는 비교적 균등하게 선택되다가 Layer 가 점점더 깊이 올라올 수록 특정 도메인에 특정 Expert 가 더 많이 선택되는 결과도 재밌다.</p>]]></content><author><name>신승윤</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[요즘 핫한 MoE 에 대해 알아보자.]]></summary></entry><entry><title type="html">LLM이 스스로 발전할 수 있을까?</title><link href="https://seungyounshin.github.io/blog/2023/SelfImprovingLLM/" rel="alternate" type="text/html" title="LLM이 스스로 발전할 수 있을까?"/><published>2023-12-30T00:00:00+09:00</published><updated>2023-12-30T00:00:00+09:00</updated><id>https://seungyounshin.github.io/blog/2023/SelfImprovingLLM</id><content type="html" xml:base="https://seungyounshin.github.io/blog/2023/SelfImprovingLLM/"><![CDATA[<h2 id="draft">Draft</h2> <h3 id="self-correction">Self Correction</h3> <p><a href="https://ar5iv.labs.arxiv.org/html/2303.11366"><strong>Reflexion</strong></a></p> <p>너무나도 유명한 논문이다. 이전에 틀렸으면 다시 반추(Reflection) 하면 원래 결과보다 계속 잘해지게 된다는 내용인데 현실적으로는 적용하기 어렵다. 왜냐하면, 우선 해당 논문에서는 Oracle Feedback 을 받는다. 예를 들어 HumanEval 과 같은 코딩문제에서는 Programming Judge 로 부터 맞았는지 틀렸는지를 알수가 있다. 밑에 <strong>Large Language Models Cannot Self-Correct Reasoning Yet</strong> 에서는 이런 Error Singal 을 흘려주는 것은 진정한 Self Correction이 아니라고 말하고 있고 나도 이 주장이 상당히 신빙성이 있다고 느껴진다. 하지만, 처음 Reflection 이라는 개념을 제안했다는 측면에서 꽤 의미 있는 논문인것 같다. 여담이지만 Neuirps 에 갔을 때 포스터에 저자가 와서 물어봤었다 ㅎㅎ 저자도 뭔가 다음 스텝을 생각하는 느낌이었다. 왜냐면 결국 성능이 Saturated 된다. 특정 성능 이상은 모델의 개별성능을 넘을 수 없다는것이다. 인간도 계속 생각하다 보면 어떤 문제는 풀 수 있지만 그래도 어떻게든 못푸는 문제가 존재하는것과 동일하다고 생각이된다.</p> <p><a href="https://arxiv.org/abs/2310.01798"><strong>Large Language Models Cannot Self-Correct Reasoning Yet</strong></a></p> <p>몰랐는데 이 논문이 ICLR2024에 Submit 되었다 결과는 어떻게 될지 모르겠지만 이 당시에는 Self Correction 하는 방법론들이 우후죽순 나오고있던 때라 Self Correction 이 단순히 Accuracy 가 올라가는것만 보면안되고 맞았다가 틀려진 것과 틀렸는데 맞아진것을 같이 보면 결국은 LLM이 아직은 Self Correcting 을 못한다는 내용이다. 근데 실제 해보면 논문의 말처럼 맞았는데 틀려지는게 꽤 된다.</p> <h3 id="synthetic-data-self-training">Synthetic Data Self-Training</h3> <p><a href="https://cdn.openai.com/papers/weak-to-strong-generalization.pdf"><strong>weak to strong(OpenAI)</strong></a></p> <p>많은 사람들이 대체 왜 그냥 Strong 모델을 훈련시키기 않냐고 비난을 받는 논문인데. 그치만 가만 생각해보면 이 세팅은 다른 어떤 논문보다 Synthetic data 를 진지하게 다루고 있다고 생각한다. 절대 이 논문의 세팅은 strong 모델을 제한하려는 방향이 아니다. weak LLM의 아웃풋 (즉, 사람이 될수도 있다) 을 가지고 이것을 $P_\text{data}$ 로 이용한다면, 결국 SFT를 하든 뭐를 하든 Global Maximum 인 사람 그 자체를 넘어서지 못한다. 근데 해당 논문에서는 이를 넘어설수 있음을 보였다. 하지만 아직 오점이 많다. 논문에서 이야기하는 strong LLM 이 사람을 넘어서는 것은 아니다 ($\texttt{MMLU} \le 90\%$) 즉, 사람을 넘어서는 아주 새로운 synthetic data가 아니라 $P_\text{data}$ 를 가지고 훈련된 underfitting 된 모델 끼리의 weak-to-strong 을 논의하고 있다.</p> <p><a href=""><strong>Reinforced Self-Training (ReST) for Language Modeling</strong></a></p> <p>Grow Step 과 Improving Step 으로 나뉘어 스스로 성장하는 LLM을만드는것인데 translation 에서만 실험을 한것이 아쉽다.</p> <p><a href="https://ar5iv.labs.arxiv.org/html/2308.08998"><strong>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</strong></a></p> <p>사람을 넘기위해서는 사람의 데이터를 모사하면 안됨. Synthetic Data 를 만들고 Scalar Feedback 을 만들어서 이걸 가지고 단순히 MMLU나 TriviaQA와 같은 언어쪽 도메인이 아니라 정말 Reasoning 이 필요한 MATH, Code(APPS, HumanEval) 에서의 정확도 향상을 보이는 논문임.</p> <p><a href="https://ar5iv.labs.arxiv.org/html/2310.10047"><strong>Improving Large Language Model Fine-tuning for Solving Math Problems</strong></a></p> <p>Pass@1 하고 Pass@K 즉 LLM 에서 여러번 Sampling 하면 성공하는 비율과 단번에 풀어버리는 비율이 수학이나 코딩쪽에서 꽤 많이 차이가 난다. 이걸 개선하는 논문인듯</p> <p><a href="https://arxiv.org/abs/2401.01335"><strong>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</strong></a></p> <p>RL로 LLM을 튜닝하는 방법은 Preference 를 필요로한다. SFT 세팅에서 같은 모델($\theta$)를 Generator, Discriminator로 생성해보고 $P_\text{data}$ 인 보통은 인간이나 GPT4가 생성한 데이터와 가까워지고 본인이 이전에 생성한것 과는 멀어지는 방향으로 학습하게된다. 수학적 수식을 통해 훈련 Loss 를 간단하게 도출한 점이 매우 훌룡한 것 같다.</p>]]></content><author><name>신승윤</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[dd]]></summary></entry><entry><title type="html">dataclasses — 데이터 클래스</title><link href="https://seungyounshin.github.io/blog/2023/python-dataclass/" rel="alternate" type="text/html" title="dataclasses — 데이터 클래스"/><published>2023-12-23T06:01:00+09:00</published><updated>2023-12-23T06:01:00+09:00</updated><id>https://seungyounshin.github.io/blog/2023/python-dataclass</id><content type="html" xml:base="https://seungyounshin.github.io/blog/2023/python-dataclass/"><![CDATA[<p>프로그래밍에서 <code class="language-plaintext highlighter-rouge">interface</code>를 잘 설계하는 것은 코드의 가독성과 유지 보수성을 높일 수 있다.</p> <p>Python 3.7 이상에서 사용할 수 있는 <code class="language-plaintext highlighter-rouge">dataclass</code>는 데코레이터를 통해 클래스 선언을 간소화할 수 있는데 또한 <code class="language-plaintext highlighter-rouge">interface</code> 로서의 역할에도 매우 좋다. 요즘 유행하는 <a href="https://docs.pydantic.dev/latest/">pydantic</a>도 이런 dataclass 에 validation 을 할 수 있는 패키지중 하나이다.</p> <h2 id="dataclass의-기본-사용법">Dataclass의 기본 사용법</h2> <p>C++의 구조체와 유사한 방식으로, Python에서도 간단한 데이터 구조를 빠르게 정의할 수 있는데 x,y 를 가지는 <code class="language-plaintext highlighter-rouge">Point</code> 클래스의 예를 만들어보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Point</span><span class="p">:</span>
    <span class="n">x</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">y</span><span class="p">:</span> <span class="nb">float</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">dataclass</code> 데코레이터는 <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__repr__</code>, <code class="language-plaintext highlighter-rouge">__eq__</code> 등의 메서드를 자동으로 추가해준다. 그렇기 때문에 인스턴스를 출력하면</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="nc">Point</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="nc">Point</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div> <p>다음과 같이 깔끔하게 포맷된 결과가 출력된다.</p> <h2 id="transformers-예시"><code class="language-plaintext highlighter-rouge">transformers</code> 예시</h2> <p>오픈소스 모델들을 쉽게 이용할 수 있는 레포인 <a href="https://github.com/huggingface/transformers">transformers</a> 에서도 <code class="language-plaintext highlighter-rouge">Argument</code> 와 모델의 인풋 아웃풋와 같은 interface 를 모두 <code class="language-plaintext highlighter-rouge">dataclass</code> 로 정의해서 사용하고있다.</p> <p>예를 들어 거의 모든 언어모델들의 아웃풋은 <a href="https://github.com/huggingface/transformers/blob/29e7a1e1834f331a4916853ecd58549ed78235d6/src/transformers/modeling_outputs.py#L25"><code class="language-plaintext highlighter-rouge">BaseModelOutput</code></a>를 상속해서 쓰는데 <code class="language-plaintext highlighter-rouge">BaseModelOutput</code> 를 보면</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseModelOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
</span><span class="gp">    ...</span>
    <span class="sh">"""</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>

</code></pre></div></div> <p>와 같이 <code class="language-plaintext highlighter-rouge">BaseModelOutput</code> 의 아웃풋은 보통의 <code class="language-plaintext highlighter-rouge">transformer</code> 구조에서 나올 수 있는 아웃풋들을 포함하는것을 알 수 있다. 이런식으로 Interface 를 datalcass 로 설계하면 우선 코드가 매우 간결해진다. 만약 ABC 클래스로 부터 이런 interface 를 만든다고 한다면 <code class="language-plaintext highlighter-rouge">__init__</code> 부터 여러가지 설계해야할점이 너무나 많고 코드가 방대해진다.</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry></feed>